
# 自定义损失函数

参考：

[CREATING EXTENSIONS USING NUMPY AND SCIPY](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html)

[Pytorch如何自定义损失函数（Loss Function）？](https://www.zhihu.com/question/66988664/answer/247952270)

[pytorch： 自定义损失函数Loss](https://blog.csdn.net/xholes/article/details/81413620)

`PyTorch`在`torch.nn`模块提供了许多常用的损失函数，同时也提供了自定义方法。以交叉熵损失（`CrossEntropyLoss`）为例

## 交叉熵损失

参考：

[交叉熵损失](https://www.zhujian.tech/posts/2626bec3.html)

[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss)

交叉熵损失用于计算预测结果和正确分类的概率损失度量

损失函数计算如下：

$$
J(\theta)= (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_{i}=j\right\} \ln p\left(y_{i}=j | x_{i}; \theta\right)
$$

$$
= (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_{i}=j\right\} \ln \frac{e^{\theta_{j}^{T} x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}
$$

其中$m$是批量大小，$k$是类别数，$y_{i}$表示每个样本$x_{i}$所属的正确类别，$1\{y_{i}=j\}$是示性函数（相等时为`1`，不相等为`0`）

求导结果如下：

$$
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \left[ 1\left\{y_{i}=s \right\} - p\left(y_{i}=s | x_{i}; \theta\right) \right]\cdot x_{i}
$$

### pytorch实现

```
import torch
import torch.nn as nn

class CustomCrossEntropyLoss(nn.Module):

    def forward(self, inputs, targets):
        assert len(inputs.shape) == 2 and len(targets) == inputs.shape[0]
        loss = 0.0
        total = torch.log(torch.sum(torch.exp(inputs), dim=1))
        batch = inputs.shape[0]
        loss = torch.sum(total - inputs[torch.arange(batch), targets])
        return loss / batch
```

### numpy实现

```
import numpy as np

class CrossEntropyLoss(object):

    def __call__(self, scores, labels):
        return self.forward(scores, labels)

    def forward(self, scores, labels):
        # scores.shape == [N, score]
        # labels.shape == [N]
        assert len(scores.shape) == 2
        assert len(labels.shape) == 1
        scores -= np.max(scores, axis=1, keepdims=True)
        expscores = np.exp(scores)
        probs = expscores / np.sum(expscores, axis=1, keepdims=True)

        N = labels.shape[0]
        correct_probs = probs[range(N), labels]
        loss = -1.0 / N * np.sum(np.log(correct_probs))
        return loss, probs

    def backward(self, probs, labels):
        assert len(probs.shape) == 2
        assert len(labels.shape) == 1
        grad_out = probs
        N = labels.shape[0]

        grad_out[range(N), labels] -= 1
        return grad_out / N
```

## 测试一

```
def pytorch_test():
    criterion = nn.CrossEntropyLoss()
    inputs = torch.Tensor([[1.1785, -0.0969, 0.5756, -1.2113, -0.1120],
                           [-0.5199, -0.8051, 1.0953, 0.1480, 0.2879],
                           [2.3401, 0.6403, 1.4306, 0.0982, -0.7363]])
    inputs.requires_grad = True
    targets = torch.Tensor([4, 3, 1]).type(torch.long)

    loss = criterion(inputs, targets)
    print(loss)
    loss.backward()
    print(inputs.grad)

    inputs.grad = None
    criterion2 = CustomCrossEntropyLoss()
    loss = criterion2.forward(inputs, targets)
    print(loss)
    loss.backward()
    print(inputs.grad)


def numpy_test():
    inputs = np.array([[1.1785, -0.0969, 0.5756, -1.2113, -0.1120],
                       [-0.5199, -0.8051, 1.0953, 0.1480, 0.2879],
                       [2.3401, 0.6403, 1.4306, 0.0982, -0.7363]])
    targets = np.array([4, 3, 1])

    criterion = CrossEntropyLoss()
    loss, probs = criterion.forward(inputs, targets)
    print(loss)
    grad = criterion.backward(probs, targets)
    print(grad)


if __name__ == '__main__':
    print('--------------------------------pytorch result')
    pytorch_test()
    print('--------------------------------numpy result')
    numpy_test()
```

结果如下：

```
--------------------------------pytorch result
tensor(2.0187, grad_fn=<NllLossBackward>)
tensor([[ 0.1520,  0.0424,  0.0832,  0.0139, -0.2915],
        [ 0.0304,  0.0228,  0.1528, -0.2741,  0.0681],
        [ 0.1918, -0.2983,  0.0772,  0.0204,  0.0088]])
tensor(2.0187, grad_fn=<DivBackward0>)
tensor([[ 0.1520,  0.0424,  0.0832,  0.0139, -0.2915],
        [ 0.0304,  0.0228,  0.1528, -0.2741,  0.0681],
        [ 0.1918, -0.2983,  0.0772,  0.0204,  0.0088]])
--------------------------------numpy result
2.018656510248812
[[ 0.15197641  0.04244993  0.0831649   0.01392834 -0.29151958]
 [ 0.0303752   0.02283802  0.15275367 -0.2740975   0.06813061]
 [ 0.19181042 -0.29828572  0.07724695  0.02038109  0.00884726]]
```

## 测试二

使用`LeNet-5`模型训练数据集`FASHION-MNIST`

```
# -*- coding: utf-8 -*-

"""
@author: zj
@file:   loss_lenet-5.py
@time:   2020-01-16
"""

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data
from torchvision.datasets import FashionMNIST
import torchvision.transforms as transforms

logging.basicConfig(format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s', level=logging.DEBUG)

def load_data():
    transform = transforms.Compose([
        transforms.Grayscale(),
        transforms.Resize(size=(32, 32)),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.5,), std=(0.5,))
    ])

    train_dataset = FashionMNIST('./data/fashionmnist/', train=True, download=True, transform=transform)
    test_dataset = FashionMNIST('./data/fashionmnist', train=False, download=True, transform=transform)

    train_dataloader = data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)
    test_dataloader = data.DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=4)

    return train_dataloader, test_dataloader


class LeNet5(nn.Module):

    def __init__(self):
        super(LeNet5, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)
        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0, bias=True)

        self.pool = nn.MaxPool2d((2, 2), stride=2)

        self.fc1 = nn.Linear(in_features=120, out_features=84, bias=True)
        self.fc2 = nn.Linear(84, 10, bias=True)

    def forward(self, input):
        x = self.pool(F.relu(self.conv1(input)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.conv3(x)

        x = x.view(-1, self.num_flat_features(x))

        x = F.relu(self.fc1(x))
        return self.fc2(x)

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


def compute_accuracy(loader, net, device):
    total_accuracy = 0
    num = 0
    for item in loader:
        data, labels = item
        data = data.to(device)
        labels = labels.to(device)

        scores = net.forward(data)
        predicted = torch.argmax(scores, dim=1)
        total_accuracy += torch.mean((predicted == labels).float()).item()
        num += 1
    return total_accuracy / num


if __name__ == '__main__':
    train_dataloader, test_dataloader = load_data()

    device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")

    net = LeNet5().to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    optimer = optim.Adam(net.parameters(), lr=1e-3)

    logging.info("开始训练")
    epoches = 5
    for i in range(epoches):
        num = 0
        total_loss = 0
        for j, item in enumerate(train_dataloader, 0):
            data, labels = item
            data = data.to(device)
            labels = labels.to(device)

            scores = net.forward(data)
            loss = criterion.forward(scores, labels)
            total_loss += loss.item()

            optimer.zero_grad()
            loss.backward()
            optimer.step()

            num += 1
        avg_loss = total_loss / num
        logging.info('epoch: %d loss: %.6f' % (i + 1, total_loss / num))
        train_accuracy = compute_accuracy(train_dataloader, net, device)
        test_accuracy = compute_accuracy(test_dataloader, net, device)
        logging.info('train accuracy: %f test accuracy: %f' % (train_accuracy, test_accuracy))
```

训练结果如下：

```
2020-01-16 16:41:08,667 loss_lenet-5.py[line:94] INFO 开始训练
2020-01-16 16:41:21,641 loss_lenet-5.py[line:114] INFO epoch: 1 loss: 0.643288
2020-01-16 16:41:26,312 loss_lenet-5.py[line:117] INFO train accuracy: 0.843500 test accuracy: 0.833267
2020-01-16 16:41:39,899 loss_lenet-5.py[line:114] INFO epoch: 2 loss: 0.401418
2020-01-16 16:41:44,489 loss_lenet-5.py[line:117] INFO train accuracy: 0.868387 test accuracy: 0.858287
2020-01-16 16:41:57,647 loss_lenet-5.py[line:114] INFO epoch: 3 loss: 0.348542
2020-01-16 16:42:02,498 loss_lenet-5.py[line:117] INFO train accuracy: 0.866899 test accuracy: 0.850672
2020-01-16 16:42:15,939 loss_lenet-5.py[line:114] INFO epoch: 4 loss: 0.318400
2020-01-16 16:42:20,790 loss_lenet-5.py[line:117] INFO train accuracy: 0.890231 test accuracy: 0.879055
2020-01-16 16:42:34,125 loss_lenet-5.py[line:114] INFO epoch: 5 loss: 0.299220
2020-01-16 16:42:39,302 loss_lenet-5.py[line:117] INFO train accuracy: 0.899481 test accuracy: 0.882812
```

使用自定义`CustomCrossEntropyLoss`替换，实现如下：

```
from custom_cross_entropy_loss import CustomCrossEntropyLoss

criterion = CustomCrossEntropyLoss().to(device)
# criterion = nn.CrossEntropyLoss().to(device)
```

训练结果如下：

```
2020-01-16 16:45:56,425 loss_lenet-5.py[line:94] INFO 开始训练
2020-01-16 16:46:01,989 loss_lenet-5.py[line:114] INFO epoch: 1 loss: 0.668920
2020-01-16 16:46:05,887 loss_lenet-5.py[line:117] INFO train accuracy: 0.834488 test accuracy: 0.827532
2020-01-16 16:46:11,163 loss_lenet-5.py[line:114] INFO epoch: 2 loss: 0.411910
2020-01-16 16:46:14,959 loss_lenet-5.py[line:117] INFO train accuracy: 0.868237 test accuracy: 0.859771
2020-01-16 16:46:20,193 loss_lenet-5.py[line:114] INFO epoch: 3 loss: 0.356210
2020-01-16 16:46:23,908 loss_lenet-5.py[line:117] INFO train accuracy: 0.881885 test accuracy: 0.871934
2020-01-16 16:46:29,045 loss_lenet-5.py[line:114] INFO epoch: 4 loss: 0.326178
2020-01-16 16:46:32,772 loss_lenet-5.py[line:117] INFO train accuracy: 0.886821 test accuracy: 0.873813
2020-01-16 16:46:38,082 loss_lenet-5.py[line:114] INFO epoch: 5 loss: 0.305682
2020-01-16 16:46:41,736 loss_lenet-5.py[line:117] INFO train accuracy: 0.897943 test accuracy: 0.883208
```

## 小结

除了使用`pytorch`实现损失函数功能外，还可以用`numpy`进行计算，再转换成`torch.Tensor`格式，不过这个时候就得同时实现前向和反向操作